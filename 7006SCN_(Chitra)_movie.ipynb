{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTq8-U9xRsdn",
        "outputId": "e5692f9e-9440-4f77-fbae-1068078e1a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Project: /content/drive/MyDrive/7006SCN_project\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pyspark==3.5.1 pyarrow==16.1.0 pandas==2.2.2 scikit-learn==1.5.1 datasets==2.20.0\n",
        "\n",
        "import os, json, time, random\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/7006SCN_project\"\n",
        "DATA_DIR    = os.path.join(PROJECT_DIR, \"data\")\n",
        "OUT_DIR     = os.path.join(PROJECT_DIR, \"outputs\")\n",
        "MODEL_DIR   = os.path.join(PROJECT_DIR, \"models\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "PARQUET_PATH = os.path.join(DATA_DIR, \"reddit_movie_sample_50k.parquet\")\n",
        "CSV_PATH     = os.path.join(DATA_DIR, \"reddit_movie_sample_50k.csv\")\n",
        "\n",
        "print(\"Project:\", PROJECT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# A) Ensure sample exists (streaming extraction if missing)\n",
        "# ============================================================\n",
        "from datasets import load_dataset\n",
        "\n",
        "def build_sample_if_missing(n_sample=50_000, seed=42):\n",
        "    if os.path.exists(PARQUET_PATH) and os.path.exists(CSV_PATH):\n",
        "        print(\"Sample already exists.\")\n",
        "        return\n",
        "\n",
        "    print(\"Sample not found -> creating via streaming (no full download)...\")\n",
        "    random.seed(seed)\n",
        "\n",
        "    ds = load_dataset(\"ZhankuiHe/reddit_movie_large_v1\", split=\"train\", streaming=True)\n",
        "\n",
        "    rows = []\n",
        "    for ex in ds:\n",
        "        # pick text\n",
        "        t = ex.get(\"processed\")\n",
        "        if t is None or str(t).strip()==\"\":\n",
        "            t = ex.get(\"raw\",\"\")\n",
        "        if t is None:\n",
        "            continue\n",
        "        t = str(t)\n",
        "        if len(t) < 3:\n",
        "            continue\n",
        "\n",
        "        # strict label\n",
        "        lab = ex.get(\"is_seeker\", None)\n",
        "        if lab is None:\n",
        "            continue\n",
        "\n",
        "        rows.append({\n",
        "            \"conv_id\": ex.get(\"conv_id\"),\n",
        "            \"turn_id\": ex.get(\"turn_id\"),\n",
        "            \"turn_order\": ex.get(\"turn_order\"),\n",
        "            \"utc_time\": ex.get(\"utc_time\"),\n",
        "            \"upvotes\": ex.get(\"upvotes\"),\n",
        "            \"text\": t,\n",
        "            \"is_seeker\": bool(lab)\n",
        "        })\n",
        "\n",
        "        if len(rows) >= n_sample:\n",
        "            break\n",
        "\n",
        "    pdf = pd.DataFrame(rows)\n",
        "    pdf[\"text\"] = pdf[\"text\"].astype(str)\n",
        "    pdf[\"is_seeker\"] = pdf[\"is_seeker\"].astype(bool)\n",
        "    pdf[\"upvotes\"] = pd.to_numeric(pdf[\"upvotes\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    pdf.to_parquet(PARQUET_PATH, index=False)\n",
        "    pdf.to_csv(CSV_PATH, index=False)\n",
        "\n",
        "    print(\"Saved sample:\")\n",
        "    print(\"PARQUET:\", PARQUET_PATH)\n",
        "    print(\"CSV:\", CSV_PATH)\n",
        "\n",
        "build_sample_if_missing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOl38dBjR82c",
        "outputId": "5b458414-a259-4700-b86d-6cc0b533d280"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# B) Load sample into Spark + strict binary label (0/1)\n",
        "# ============================================================\n",
        "from pyspark.sql.functions import col, when, length\n",
        "\n",
        "df = spark.read.parquet(PARQUET_PATH)\n",
        "\n",
        "# Clean + enforce binary numeric label\n",
        "df = (\n",
        "    df\n",
        "    .withColumn(\"text\", col(\"text\").cast(\"string\"))\n",
        "    .withColumn(\"is_seeker\", col(\"is_seeker\").cast(\"boolean\"))\n",
        "    .filter(col(\"is_seeker\").isNotNull())\n",
        "    .withColumn(\"label\", when(col(\"is_seeker\") == True, 1.0).otherwise(0.0))  # STRICT 0/1\n",
        "    .withColumn(\"upvotes\", when(col(\"upvotes\").isNull(), 0.0).otherwise(col(\"upvotes\").cast(\"double\")))\n",
        "    .filter(length(col(\"text\")) >= 3)\n",
        ")\n",
        "\n",
        "# Sanity check: label cardinality MUST be 2\n",
        "print(\"Distinct label values:\", [r[\"label\"] for r in df.select(\"label\").distinct().collect()])\n",
        "\n",
        "# Partition + cache\n",
        "if \"conv_id\" in df.columns:\n",
        "    df = df.repartition(32, \"conv_id\")\n",
        "else:\n",
        "    df = df.repartition(32)\n",
        "\n",
        "df = df.persist()\n",
        "print(\"Rows:\", df.count(), \"Cols:\", len(df.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y49n7L7bSBKU",
        "outputId": "055f59f7-ea1f-49d7-fb8c-6c67a7e78f21"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct label values: [1.0, 0.0]\n",
            "Rows: 50000 Cols: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# C) Train/test split\n",
        "# ============================================================\n",
        "SEED = 42\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=SEED)\n",
        "train_df = train_df.persist()\n",
        "test_df  = test_df.persist()\n",
        "print(\"Train:\", train_df.count(), \"Test:\", test_df.count())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzhMTDkkSHGb",
        "outputId": "23ef8d28-0a64-4fa6-fcbe-8518aea17d16"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 40076 Test: 9924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# D) Feature pipeline (text -> features)\n",
        "# ============================================================\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\", minTokenLength=2)\n",
        "stop      = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_clean\")\n",
        "tf        = HashingTF(inputCol=\"tokens_clean\", outputCol=\"tf\", numFeatures=1<<18)\n",
        "idf       = IDF(inputCol=\"tf\", outputCol=\"features\")"
      ],
      "metadata": {
        "id": "R9n0VFVoXDIb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# D) Feature pipeline (text -> features)\n",
        "# ============================================================\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\", minTokenLength=2)\n",
        "stop      = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_clean\")\n",
        "tf        = HashingTF(inputCol=\"tokens_clean\", outputCol=\"tf\", numFeatures=1<<18)\n",
        "idf       = IDF(inputCol=\"tf\", outputCol=\"features\")\n"
      ],
      "metadata": {
        "id": "8vlB1RGJXG48"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# E) Models (4) + evaluation\n",
        "# ============================================================\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes, LinearSVC, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "\n",
        "def compute_metrics(pred_df):\n",
        "    auc = evaluator_auc.evaluate(pred_df)\n",
        "\n",
        "    pred_and_labels = pred_df.select(\n",
        "        col(\"prediction\").cast(\"double\"),\n",
        "        col(\"label\").cast(\"double\")\n",
        "    ).rdd.map(tuple)\n",
        "\n",
        "    mm = MulticlassMetrics(pred_and_labels)\n",
        "    return {\n",
        "        \"auc\": float(auc),\n",
        "        \"accuracy\": float(mm.accuracy),\n",
        "        \"precision\": float(mm.weightedPrecision),\n",
        "        \"recall\": float(mm.weightedRecall),\n",
        "        \"f1\": float(mm.weightedFMeasure())\n",
        "    }\n",
        "\n",
        "def fit_and_eval(model, model_name):\n",
        "    pipeline = Pipeline(stages=[tokenizer, stop, tf, idf, model])\n",
        "\n",
        "    t0 = time.time()\n",
        "    fitted = pipeline.fit(train_df)\n",
        "    fit_seconds = time.time() - t0\n",
        "\n",
        "    select_cols = [\"label\", \"prediction\", \"rawPrediction\"]\n",
        "    # LinearSVC does not produce a 'probability' column\n",
        "    if model_name != \"LinearSVC\":\n",
        "        select_cols.append(\"probability\")\n",
        "\n",
        "    preds = fitted.transform(test_df).select(*select_cols)\n",
        "    metrics = compute_metrics(preds)\n",
        "    metrics[\"model\"] = model_name\n",
        "    metrics[\"fit_seconds\"] = float(fit_seconds)\n",
        "    return metrics, fitted\n",
        "\n",
        "# Fast settings for Colab\n",
        "lr  = LogisticRegression(maxIter=30, regParam=0.05, elasticNetParam=0.0, featuresCol=\"features\", labelCol=\"label\")\n",
        "nb  = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol=\"features\", labelCol=\"label\")\n",
        "svm = LinearSVC(maxIter=30, regParam=0.1, featuresCol=\"features\", labelCol=\"label\")\n",
        "rf  = RandomForestClassifier(numTrees=80, maxDepth=10, featureSubsetStrategy=\"sqrt\",\n",
        "                             featuresCol=\"features\", labelCol=\"label\", seed=SEED)\n",
        "\n",
        "results = []\n",
        "fitted_models = {}\n",
        "\n",
        "for name, mdl in [\n",
        "    (\"LogisticRegression\", lr),\n",
        "    (\"NaiveBayes\", nb),\n",
        "    (\"LinearSVC\", svm),\n",
        "    (\"RandomForest\", rf),\n",
        "]:\n",
        "    print(f\"\\n--- Training {name} ---\")\n",
        "    m, fitted = fit_and_eval(mdl, name)\n",
        "    results.append(m)\n",
        "    fitted_models[name] = fitted\n",
        "    print({k: round(v, 4) if isinstance(v, float) else v for k, v in m.items()})\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"f1\", ascending=False)\n",
        "print(\"\\n=== PySpark Model Comparison (sorted by F1) ===\")\n",
        "print(results_df)\n",
        "\n",
        "METRICS_CSV  = os.path.join(OUT_DIR, \"pyspark_model_metrics.csv\")\n",
        "METRICS_JSON = os.path.join(OUT_DIR, \"pyspark_model_metrics.json\")\n",
        "results_df.to_csv(METRICS_CSV, index=False)\n",
        "with open(METRICS_JSON, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\nSaved PySpark metrics:\", METRICS_CSV, METRICS_JSON)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTIG72HAXKLL",
        "outputId": "fcd11ed4-8cfd-4b92-e594-bdfcde0020bf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training LogisticRegression ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'auc': 0.9996, 'accuracy': 0.9947, 'precision': 0.9947, 'recall': 0.9947, 'f1': 0.9947, 'model': 'LogisticRegression', 'fit_seconds': 51.2862}\n",
            "\n",
            "--- Training NaiveBayes ---\n",
            "{'auc': 0.3208, 'accuracy': 0.9185, 'precision': 0.918, 'recall': 0.9185, 'f1': 0.9181, 'model': 'NaiveBayes', 'fit_seconds': 6.0598}\n",
            "\n",
            "--- Training LinearSVC ---\n",
            "{'auc': 1.0, 'accuracy': 0.9998, 'precision': 0.9998, 'recall': 0.9998, 'f1': 0.9998, 'model': 'LinearSVC', 'fit_seconds': 59.0462}\n",
            "\n",
            "--- Training RandomForest ---\n",
            "{'auc': 0.9787, 'accuracy': 0.72, 'precision': 0.8035, 'recall': 0.72, 'f1': 0.6473, 'model': 'RandomForest', 'fit_seconds': 1075.0993}\n",
            "\n",
            "=== PySpark Model Comparison (sorted by F1) ===\n",
            "        auc  accuracy  precision    recall        f1               model  \\\n",
            "2  1.000000  0.999798   0.999799  0.999798  0.999798           LinearSVC   \n",
            "0  0.999608  0.994659   0.994691  0.994659  0.994650  LogisticRegression   \n",
            "1  0.320770  0.918480   0.918050  0.918480  0.918114          NaiveBayes   \n",
            "3  0.978748  0.719972   0.803541  0.719972  0.647326        RandomForest   \n",
            "\n",
            "   fit_seconds  \n",
            "2    59.046218  \n",
            "0    51.286240  \n",
            "1     6.059794  \n",
            "3  1075.099347  \n",
            "\n",
            "Saved PySpark metrics: /content/drive/MyDrive/7006SCN_project/outputs/pyspark_model_metrics.csv /content/drive/MyDrive/7006SCN_project/outputs/pyspark_model_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# F) CrossValidator tuning (small grid) — safe on Colab\n",
        "# ============================================================\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "tune_lr = LogisticRegression(maxIter=30, featuresCol=\"features\", labelCol=\"label\")\n",
        "tune_pipeline = Pipeline(stages=[tokenizer, stop, tf, idf, tune_lr])\n",
        "\n",
        "param_grid = (ParamGridBuilder()\n",
        "              .addGrid(tune_lr.regParam, [0.1, 0.03])\n",
        "              .addGrid(tune_lr.elasticNetParam, [0.0, 0.5])\n",
        "              .build())\n",
        "\n",
        "crossval = CrossValidator(\n",
        "    estimator=tune_pipeline,\n",
        "    estimatorParamMaps=param_grid,\n",
        "    evaluator=evaluator_auc,\n",
        "    numFolds=3,\n",
        "    parallelism=2\n",
        ")\n",
        "\n",
        "print(\"\\n--- Running CrossValidator (LR small grid) ---\")\n",
        "t0 = time.time()\n",
        "cv_model = crossval.fit(train_df)\n",
        "cv_seconds = time.time() - t0\n",
        "\n",
        "cv_preds = cv_model.transform(test_df).select(\"label\", \"prediction\", \"rawPrediction\", \"probability\")\n",
        "cv_metrics = compute_metrics(cv_preds)\n",
        "cv_metrics.update({\"model\": \"LogisticRegression_CV\", \"fit_seconds\": float(cv_seconds)})\n",
        "print(\"CV metrics:\", {k: round(v, 4) if isinstance(v, float) else v for k, v in cv_metrics.items()})\n",
        "\n",
        "results.append(cv_metrics)\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"f1\", ascending=False)\n",
        "results_df.to_csv(METRICS_CSV, index=False)\n",
        "with open(METRICS_JSON, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iaz4wjSXNNS",
        "outputId": "81f6fc10-e5a2-416a-8df8-f6753dbe27b4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running CrossValidator (LR small grid) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV metrics: {'auc': 0.9998, 'accuracy': 0.999, 'precision': 0.999, 'recall': 0.999, 'f1': 0.999, 'model': 'LogisticRegression_CV', 'fit_seconds': 525.1831}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# G) Save best model + predictions\n",
        "# ============================================================\n",
        "best_row = results_df.iloc[0].to_dict()\n",
        "best_name = best_row[\"model\"]\n",
        "print(\"\\nBest model by F1:\", best_name)\n",
        "\n",
        "BEST_MODEL_PATH = os.path.join(MODEL_DIR, f\"best_model_{best_name}\")\n",
        "best_obj = cv_model if best_name == \"LogisticRegression_CV\" else fitted_models[best_name]\n",
        "best_obj.write().overwrite().save(BEST_MODEL_PATH)\n",
        "print(\"Saved best Spark model to:\", BEST_MODEL_PATH)\n",
        "\n",
        "# Predictions for Tableau/report\n",
        "select_cols = [\n",
        "    \"conv_id\", \"turn_id\", \"turn_order\", \"utc_time\", \"upvotes\",\n",
        "    col(\"text\").alias(\"text\"),\n",
        "    col(\"label\").alias(\"label\"),\n",
        "    col(\"prediction\").alias(\"prediction\")\n",
        "]\n",
        "\n",
        "# Only add 'probability' if the model produces it (e.g., not LinearSVC)\n",
        "if best_name != \"LinearSVC\":\n",
        "    select_cols.append(\"probability\")\n",
        "\n",
        "pred_export = best_obj.transform(test_df).select(*select_cols)\n",
        "\n",
        "PRED_PARQUET = os.path.join(OUT_DIR, \"test_predictions.parquet\")\n",
        "PRED_CSV     = os.path.join(OUT_DIR, \"test_predictions_sample.csv\")\n",
        "\n",
        "pred_export.write.mode(\"overwrite\").parquet(PRED_PARQUET)\n",
        "pred_export.limit(5000).toPandas().to_csv(PRED_CSV, index=False)\n",
        "\n",
        "print(\"Saved predictions parquet:\", PRED_PARQUET)\n",
        "print(\"Saved predictions CSV sample:\", PRED_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do4YbWPbXP0r",
        "outputId": "69759943-7b89-4db9-a479-d51466387886"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best model by F1: LinearSVC\n",
            "Saved best Spark model to: /content/drive/MyDrive/7006SCN_project/models/best_model_LinearSVC\n",
            "Saved predictions parquet: /content/drive/MyDrive/7006SCN_project/outputs/test_predictions.parquet\n",
            "Saved predictions CSV sample: /content/drive/MyDrive/7006SCN_project/outputs/test_predictions_sample.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# H) sklearn baseline (single node) — TFIDF + LogisticRegression\n",
        "# ============================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression as SkLR\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "pdf = pd.read_parquet(PARQUET_PATH)\n",
        "X = pdf[\"text\"].astype(str).values\n",
        "y = pdf[\"is_seeker\"].astype(bool).astype(int).values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=200_000, ngram_range=(1,2))\n",
        "Xtr = tfidf.fit_transform(X_train)\n",
        "Xte = tfidf.transform(X_test)\n",
        "\n",
        "sk = SkLR(max_iter=200, n_jobs=-1)\n",
        "t0 = time.time()\n",
        "sk.fit(Xtr, y_train)\n",
        "sk_seconds = time.time() - t0\n",
        "\n",
        "proba = sk.predict_proba(Xte)[:, 1]\n",
        "pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "acc = accuracy_score(y_test, pred)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(y_test, pred, average=\"weighted\")\n",
        "auc = roc_auc_score(y_test, proba)\n",
        "\n",
        "sk_metrics = {\n",
        "    \"model\": \"sklearn_TFIDF_LogReg\",\n",
        "    \"auc\": float(auc),\n",
        "    \"accuracy\": float(acc),\n",
        "    \"precision\": float(prec),\n",
        "    \"recall\": float(rec),\n",
        "    \"f1\": float(f1),\n",
        "    \"fit_seconds\": float(sk_seconds)\n",
        "}\n",
        "\n",
        "SK_JSON = os.path.join(OUT_DIR, \"sklearn_baseline_metrics.json\")\n",
        "with open(SK_JSON, \"w\") as f:\n",
        "    json.dump(sk_metrics, f, indent=2)\n",
        "\n",
        "print(\"\\n=== sklearn baseline ===\")\n",
        "print({k: round(v, 4) if isinstance(v, float) else v for k, v in sk_metrics.items()})\n",
        "print(\"Saved sklearn baseline metrics:\", SK_JSON)\n",
        "\n",
        "# Append sklearn baseline to main metrics too\n",
        "results.append(sk_metrics)\n",
        "final_df = pd.DataFrame(results).sort_values(by=\"f1\", ascending=False)\n",
        "final_df.to_csv(METRICS_CSV, index=False)\n",
        "with open(METRICS_JSON, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\nFINAL OUTPUTS saved to Drive:\")\n",
        "print(\"- Sample Parquet:\", PARQUET_PATH)\n",
        "print(\"- Sample CSV:\", CSV_PATH)\n",
        "print(\"- Metrics CSV/JSON:\", METRICS_CSV, METRICS_JSON)\n",
        "print(\"- Best model folder:\", BEST_MODEL_PATH)\n",
        "print(\"- Predictions:\", PRED_PARQUET, PRED_CSV)\n",
        "\n",
        "# ---------------------------\n",
        "# Unpersist (good practice)\n",
        "# ---------------------------\n",
        "train_df.unpersist()\n",
        "test_df.unpersist()\n",
        "df.unpersist()\n",
        "\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCRIOSeLXS71",
        "outputId": "84fdf05d-92b1-4e3e-ded9-b1d395df16b2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== sklearn baseline ===\n",
            "{'model': 'sklearn_TFIDF_LogReg', 'auc': 1.0, 'accuracy': 0.9966, 'precision': 0.9966, 'recall': 0.9966, 'f1': 0.9966, 'fit_seconds': 3.5407}\n",
            "Saved sklearn baseline metrics: /content/drive/MyDrive/7006SCN_project/outputs/sklearn_baseline_metrics.json\n",
            "\n",
            "FINAL OUTPUTS saved to Drive:\n",
            "- Sample Parquet: /content/drive/MyDrive/7006SCN_project/data/reddit_movie_sample_50k.parquet\n",
            "- Sample CSV: /content/drive/MyDrive/7006SCN_project/data/reddit_movie_sample_50k.csv\n",
            "- Metrics CSV/JSON: /content/drive/MyDrive/7006SCN_project/outputs/pyspark_model_metrics.csv /content/drive/MyDrive/7006SCN_project/outputs/pyspark_model_metrics.json\n",
            "- Best model folder: /content/drive/MyDrive/7006SCN_project/models/best_model_LinearSVC\n",
            "- Predictions: /content/drive/MyDrive/7006SCN_project/outputs/test_predictions.parquet /content/drive/MyDrive/7006SCN_project/outputs/test_predictions_sample.csv\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}